# 数据准备

对数据使用[[jieba]]分词（中文），将分词结果合并为字符串，以空格分隔
```python
words[:3]
```
output:
```python
['信托公司 2031 年 上半年 经营 业绩 概览', '首单 信托 型 企业 ABS 获批', '华能 贵 诚信 托孙磊 : 金融 科技 助力 打造 开放 信托 生态']
```

# 使用sklearn方法进行文本向量化

## 获取数组结果
```python
# 根据词频，进行 文本向量化
from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()
X = vect.fit_transform(words)  # 将words进行词频向量化
X = X.toarray()  # 将结果转换为数组
X
```

## 将结果组装成DataFrame
```python
# 将结果组装成DataFrame
all_words_bag = vect.get_feature_names_out()  # 获取所有新闻标题的词袋
df = pd.DataFrame(X, columns=all_words_bag)
df.head()
```
output:

| |00700|03|04|08s|09|10|100|11|12|150|...|黄萍|黄金|黑客|黑灰产|黑金|黑马|鼓手|鼻祖|齐聚|龙风|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0|0|0|0|0|0|0|0|0|0|0|...|0|0|0|0|0|0|0|0|0|0|
|1|0|0|0|0|0|0|0|0|0|0|...|0|0|0|0|0|0|0|0|0|0|
|2|0|0|0|0|0|0|0|0|0|0|...|0|0|0|0|0|0|0|0|0|0|
|3|0|0|0|0|0|0|0|0|0|0|...|0|0|0|0|0|0|0|0|0|0|
|4|0|0|0|0|0|0|0|0|0|0|...|0|0|0|0|0|0|0|0|0|0|
5 rows × 3402 columns

>解释：
>行：每条新闻标题（数据）中，这个词出现的次数

### 词袋

```python
# 查看提取出的词袋
all_words_bag
```
output:
```python
array(['00700', '03', '04', ..., '鼻祖', '齐聚', '龙风'], dtype=object)
```
